{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=19.33s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco = COCO(annotation_file=\"/scratch/lt2316-h18-resources/coco/annotations/instances_train2017.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cat = coco.getCatIds(catNms=\"cat\")\n",
    "horse_cat = coco.getCatIds(catNms=\"horse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([17], [19])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cat, horse_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_imgs = coco.getImgIds(catIds=cat_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "horse_imgs = coco.getImgIds(catIds=horse_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(cat_imgs)\n",
    "cat_imgs_train = cat_imgs[:2200]\n",
    "cat_imgs_val = cat_imgs[2200:2420]\n",
    "cat_imgs_test = cat_imgs[2420:2640]\n",
    "\n",
    "\n",
    "random.shuffle(horse_imgs)\n",
    "horse_imgs_train = horse_imgs[:2200]\n",
    "horse_imgs_test = horse_imgs[2200:2420]\n",
    "horse_imgs_val = horse_imgs[2420:2640]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2200, 220)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_imgs_train), len(cat_imgs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in cat_imgs[0:600] if x in horse_imgs[0:600]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_meta_train = coco.loadImgs(ids=cat_imgs_train)\n",
    "cat_meta_test = coco.loadImgs(ids=cat_imgs_test)\n",
    "horse_meta_train = coco.loadImgs(ids=horse_imgs_train)\n",
    "horse_meta_test = coco.loadImgs(ids=horse_imgs_test)\n",
    "cat_meta_val = coco.loadImgs(ids=cat_imgs_val)\n",
    "horse_meta_val = coco.loadImgs(ids=horse_imgs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(meta, datadir=\"/scratch/lt2316-h18-resources/coco/train2017\"):\n",
    "    return [(x['file_name'], Image.open(\"{}/{}\".format(datadir, x['file_name'])).resize((100,100))) for x in meta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_data_train = get_data(cat_meta_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2200"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "horse_data_train = get_data(horse_meta_train)\n",
    "cat_data_test = get_data(cat_meta_test)\n",
    "horse_data_test = get_data(horse_meta_test)\n",
    "cat_data_val = get_data(cat_meta_val)\n",
    "horse_data_val = get_data(horse_meta_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_data_train_df = pd.DataFrame(cat_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_data_train_df['class'] = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "horse_data_train_df = pd.DataFrame(horse_data_train)\n",
    "horse_data_train_df['class'] = 'horse'\n",
    "\n",
    "cat_data_test_df = pd.DataFrame(cat_data_test)\n",
    "cat_data_test_df['class'] = 'cat'\n",
    "\n",
    "horse_data_test_df = pd.DataFrame(horse_data_test)\n",
    "horse_data_test_df['class'] = 'horse'\n",
    "\n",
    "cat_data_val_df = pd.DataFrame(cat_data_val)\n",
    "cat_data_val_df['class'] = 'cat'\n",
    "\n",
    "horse_data_val_df = pd.DataFrame(horse_data_val)\n",
    "horse_data_val_df['class'] = 'horse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([cat_data_train_df, horse_data_train_df])\n",
    "test_df = pd.concat([cat_data_test_df, horse_data_test_df])\n",
    "val_df =pd.concat([cat_data_val_df, horse_data_val_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>imgs</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000262727.jpg</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=100x100 a...</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000000185030.jpg</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=100x100 a...</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000000160743.jpg</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=100x100 a...</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000000343552.jpg</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=100x100 a...</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000000342244.jpg</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=100x100 a...</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185</th>\n",
       "      <td>000000279550.jpg</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=100x100 a...</td>\n",
       "      <td>horse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2186</th>\n",
       "      <td>000000303227.jpg</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=100x100 a...</td>\n",
       "      <td>horse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2187</th>\n",
       "      <td>000000129072.jpg</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=100x100 a...</td>\n",
       "      <td>horse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2188</th>\n",
       "      <td>000000346849.jpg</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=100x100 a...</td>\n",
       "      <td>horse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2189</th>\n",
       "      <td>000000085827.jpg</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=100x100 a...</td>\n",
       "      <td>horse</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4390 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Path                                               imgs  \\\n",
       "0     000000262727.jpg  <PIL.Image.Image image mode=RGB size=100x100 a...   \n",
       "1     000000185030.jpg  <PIL.Image.Image image mode=RGB size=100x100 a...   \n",
       "2     000000160743.jpg  <PIL.Image.Image image mode=RGB size=100x100 a...   \n",
       "3     000000343552.jpg  <PIL.Image.Image image mode=RGB size=100x100 a...   \n",
       "4     000000342244.jpg  <PIL.Image.Image image mode=RGB size=100x100 a...   \n",
       "...                ...                                                ...   \n",
       "2185  000000279550.jpg  <PIL.Image.Image image mode=RGB size=100x100 a...   \n",
       "2186  000000303227.jpg  <PIL.Image.Image image mode=RGB size=100x100 a...   \n",
       "2187  000000129072.jpg  <PIL.Image.Image image mode=RGB size=100x100 a...   \n",
       "2188  000000346849.jpg  <PIL.Image.Image image mode=RGB size=100x100 a...   \n",
       "2189  000000085827.jpg  <PIL.Image.Image image mode=RGB size=100x100 a...   \n",
       "\n",
       "      label  \n",
       "0       cat  \n",
       "1       cat  \n",
       "2       cat  \n",
       "3       cat  \n",
       "4       cat  \n",
       "...     ...  \n",
       "2185  horse  \n",
       "2186  horse  \n",
       "2187  horse  \n",
       "2188  horse  \n",
       "2189  horse  \n",
       "\n",
       "[4390 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.rename(columns={0: 'Path', 1: 'imgs', 'class':'label'}, inplace=True)\n",
    "test_df.rename(columns={0: 'Path', 1: 'imgs', 'class':'label'}, inplace=True)\n",
    "val_df.rename(columns={0: 'Path', 1: 'imgs', 'class':'label'}, inplace=True)\n",
    "train_df.head(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat' 'horse']\n"
     ]
    }
   ],
   "source": [
    "# Geting unique labels\n",
    "labels = train_df['label'].unique()\n",
    "\n",
    "print(labels)\n",
    "\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "\n",
    "size = (\n",
    "\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "\n",
    ")\n",
    "\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class CustomDatasetCOCO(Dataset):\n",
    "    def __init__(self, dataframe, transforms=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transforms = transforms\n",
    "        self.dataframe['label'] = self.dataframe['label'].apply(lambda x: 0 if x == 'cat' else 1)\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataframe.iloc[idx]['imgs']\n",
    "        label = torch.tensor(self.dataframe.iloc[idx]['label'], dtype=torch.long)\n",
    "        \n",
    "        # Ensuring the image is RGB\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return {'pixel_values': image, 'label': label}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = CustomDatasetCOCO(dataframe=train_df, transforms=_transforms)\n",
    "dataset_test = CustomDatasetCOCO(dataframe=test_df, transforms=_transforms)\n",
    "dataset_val = CustomDatasetCOCO(dataframe=val_df, transforms=_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample keys: dict_keys(['pixel_values', 'label'])\n",
      "Image type: <class 'torch.Tensor'>\n",
      "Label type: <class 'torch.Tensor'>\n",
      "Label: tensor(1)\n",
      "Image mode: RGB\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "sample = dataset_test[-10]\n",
    "print(\"Sample keys:\", sample.keys())\n",
    "print(\"Image type:\", type(sample['pixel_values']))\n",
    "print(\"Label type:\", type(sample['label']))\n",
    "print(\"Label:\", sample['label'])\n",
    "\n",
    "image = sample['pixel_values']\n",
    "\n",
    "# Converting from torch tensor back to PIL image for checking the mode\n",
    "image = torchvision.transforms.ToPILImage()(image)\n",
    "\n",
    "print(\"Image mode:\", image.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = int(len(id2label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'cat', '1': 'horse'}\n"
     ]
    }
   ],
   "source": [
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "\n",
    "    checkpoint,\n",
    "\n",
    "    num_labels=len(labels),\n",
    "\n",
    "    id2label=id2label,\n",
    "\n",
    "    label2id=label2id,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guslasbo@GU.GU.SE/miniconda3/envs/Assignment1_ML/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102' max='102' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [102/102 02:52, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.230800</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.965909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.077800</td>\n",
       "      <td>0.083130</td>\n",
       "      <td>0.981818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guslasbo@GU.GU.SE/miniconda3/envs/Assignment1_ML/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/guslasbo@GU.GU.SE/miniconda3/envs/Assignment1_ML/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=102, training_loss=0.20783318549978966, metrics={'train_runtime': 178.4335, 'train_samples_per_second': 73.977, 'train_steps_per_second': 0.572, 'total_flos': 1.0092556727404462e+18, 'train_loss': 0.20783318549978966, 'epoch': 2.96})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "    output_dir=\"my_awesome_cat_model\",\n",
    "\n",
    "    remove_unused_columns=False,\n",
    "\n",
    "    evaluation_strategy=\"epoch\",\n",
    "\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    learning_rate=5e-5,\n",
    "\n",
    "    per_device_train_batch_size=16,\n",
    "\n",
    "    gradient_accumulation_steps=4,\n",
    "\n",
    "    per_device_eval_batch_size=16,\n",
    "\n",
    "    num_train_epochs=3,\n",
    "\n",
    "    warmup_ratio=0.1,\n",
    "\n",
    "    logging_steps=10,\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "\n",
    "    push_to_hub=False,\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = val_df\n",
    "\n",
    "image = ds.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path                                      000000499054.jpg\n",
      "imgs     <PIL.Image.Image image mode=RGB size=100x100 a...\n",
      "label                                                    0\n",
      "Name: 5, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(ds.iloc[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=100x100 at 0x7F7A61044550>\n"
     ]
    }
   ],
   "source": [
    "print(image['imgs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9908796548843384, 'label': 'horse'},\n",
       " {'score': 0.009120305068790913, 'label': 'cat'}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"image-classification\", model=\"./my_awesome_cat_model/checkpoint-93\")\n",
    "\n",
    "classifier(image['imgs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "import torch\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"./my_awesome_cat_model/checkpoint-93\")\n",
    "\n",
    "inputs = image_processor(image['imgs'], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\"./my_awesome_cat_model/checkpoint-93\")\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'horse'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_label = logits.argmax(-1).item()\n",
    "\n",
    "model.config.id2label[predicted_label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
